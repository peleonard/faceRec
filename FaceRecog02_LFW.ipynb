{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Labeled Faces in the Wild\n",
    "\n",
    "A benchmark dataset for Face Recognition\n",
    "\n",
    "http://vis-www.cs.umass.edu/lfw/\n",
    "\n",
    "...a database of face photographs designed for studying the problem of unconstrained face recognition. The data set contains more than 13,000 images of faces collected from the web. Each face has been labeled with the name of the person pictured. 1680 of the people pictured have two or more distinct photos in the data set. The only constraint on these faces is that they were detected by the Viola-Jones face detector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total dataset size:\n",
      "n_samples: 4324\n",
      "n_features: 46875\n",
      "n_classes: 158\n",
      "image size: 125, 125, 3\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "from time import time\n",
    "import logging\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n",
    "\n",
    "# #############################################################################\n",
    "# Download the data, if not already on disk and load it as numpy arrays\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=10, color=True, slice_ = None)\n",
    "\n",
    "# introspect the images arrays to find the shapes (for plotting)\n",
    "n_samples, h, w, c = lfw_people.images.shape\n",
    "\n",
    "X = lfw_people.data\n",
    "n_features = X.shape[1]\n",
    "\n",
    "# the label to predict is the id of the person\n",
    "y = lfw_people.target\n",
    "target_names = lfw_people.target_names\n",
    "n_classes = target_names.shape[0]\n",
    "\n",
    "print(\"n_samples: %d\" % n_samples)\n",
    "print(\"n_features: %d\" % n_features)\n",
    "print(\"n_classes: %d\" % n_classes)\n",
    "print(\"image size: %d, %d, %d\" %(h,w,c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train shape: (3243, 125, 125, 3)\n",
      "3243 train samples\n",
      "1081 test samples\n",
      "number of classes:  158\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "import os\n",
    "\n",
    "num_classes = n_classes\n",
    "# The data, split between train and test sets:\n",
    "x_train, x_test, y_train, y_test = train_test_split(lfw_people.images, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "print('x_train shape:', x_train.shape)\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')\n",
    "print('number of classes: ', num_classes)\n",
    "\n",
    "# Convert class vectors to binary class matrices.\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tb_callback = keras.callbacks.TensorBoard(log_dir='./Graph', histogram_freq=0,  \n",
    "          write_graph=True, write_images=True)\n",
    "\n",
    "batch_size = 50\n",
    "\n",
    "epochs = 200\n",
    "data_augmentation = True\n",
    "save_dir = os.path.join(os.getcwd(), 'saved_models')\n",
    "model_name = 'keras_lfw_trained_model.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using real-time data augmentation.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2018-05-07 09:32:06,574 Variable *= will be deprecated. Use variable.assign_mul if you want assignment to the variable value or 'x = x * y' if you want a new python Tensor object.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "65/65 [==============================] - 276s 4s/step - loss: 4.7063 - acc: 0.1123 - val_loss: 4.7128 - val_acc: 0.1295\n",
      "Epoch 2/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 4.6064 - acc: 0.1147 - val_loss: 4.6429 - val_acc: 0.1295\n",
      "Epoch 3/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 4.5386 - acc: 0.1173 - val_loss: 4.5524 - val_acc: 0.1341\n",
      "Epoch 4/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 4.4735 - acc: 0.1192 - val_loss: 4.4931 - val_acc: 0.1369\n",
      "Epoch 5/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 4.3938 - acc: 0.1215 - val_loss: 4.3503 - val_acc: 0.1378\n",
      "Epoch 6/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 4.3335 - acc: 0.1280 - val_loss: 4.2972 - val_acc: 0.1563\n",
      "Epoch 7/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 4.2690 - acc: 0.1307 - val_loss: 4.2044 - val_acc: 0.1656\n",
      "Epoch 8/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 4.1782 - acc: 0.1368 - val_loss: 4.1932 - val_acc: 0.1767\n",
      "Epoch 9/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 4.1200 - acc: 0.1493 - val_loss: 4.0813 - val_acc: 0.1721\n",
      "Epoch 10/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 4.0453 - acc: 0.1623 - val_loss: 3.9976 - val_acc: 0.1943\n",
      "Epoch 11/200\n",
      "65/65 [==============================] - 272s 4s/step - loss: 3.9927 - acc: 0.1674 - val_loss: 3.9336 - val_acc: 0.2146\n",
      "Epoch 12/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 3.9068 - acc: 0.1695 - val_loss: 3.8925 - val_acc: 0.2118\n",
      "Epoch 13/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 3.8668 - acc: 0.1865 - val_loss: 3.8119 - val_acc: 0.2294\n",
      "Epoch 14/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 3.7990 - acc: 0.1859 - val_loss: 3.7675 - val_acc: 0.2470\n",
      "Epoch 15/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 3.7146 - acc: 0.2033 - val_loss: 3.7443 - val_acc: 0.2488\n",
      "Epoch 16/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 3.6816 - acc: 0.2120 - val_loss: 3.6870 - val_acc: 0.2479\n",
      "Epoch 17/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 3.5943 - acc: 0.2187 - val_loss: 3.6420 - val_acc: 0.2636\n",
      "Epoch 18/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 3.5410 - acc: 0.2177 - val_loss: 3.6067 - val_acc: 0.2368\n",
      "Epoch 19/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 3.5124 - acc: 0.2211 - val_loss: 3.5272 - val_acc: 0.2609\n",
      "Epoch 20/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 3.4346 - acc: 0.2370 - val_loss: 3.4948 - val_acc: 0.2710\n",
      "Epoch 21/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 3.3864 - acc: 0.2492 - val_loss: 3.4384 - val_acc: 0.2840\n",
      "Epoch 22/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 3.3209 - acc: 0.2582 - val_loss: 3.3968 - val_acc: 0.2979\n",
      "Epoch 23/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 3.2520 - acc: 0.2632 - val_loss: 3.4045 - val_acc: 0.2905\n",
      "Epoch 24/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 3.1983 - acc: 0.2753 - val_loss: 3.3722 - val_acc: 0.2979\n",
      "Epoch 25/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 3.1548 - acc: 0.2832 - val_loss: 3.2907 - val_acc: 0.3127\n",
      "Epoch 26/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 3.1462 - acc: 0.2743 - val_loss: 3.2303 - val_acc: 0.3247\n",
      "Epoch 27/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 3.0669 - acc: 0.2969 - val_loss: 3.1897 - val_acc: 0.3451\n",
      "Epoch 28/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 3.0491 - acc: 0.2934 - val_loss: 3.1939 - val_acc: 0.3395\n",
      "Epoch 29/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 3.0084 - acc: 0.3095 - val_loss: 3.1383 - val_acc: 0.3460\n",
      "Epoch 30/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 2.9655 - acc: 0.3135 - val_loss: 3.0910 - val_acc: 0.3451\n",
      "Epoch 31/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 2.8837 - acc: 0.3389 - val_loss: 3.0850 - val_acc: 0.3663\n",
      "Epoch 32/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 2.8195 - acc: 0.3299 - val_loss: 3.0463 - val_acc: 0.3756\n",
      "Epoch 33/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 2.8260 - acc: 0.3441 - val_loss: 3.0289 - val_acc: 0.3645\n",
      "Epoch 34/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 2.7790 - acc: 0.3384 - val_loss: 3.0257 - val_acc: 0.3515\n",
      "Epoch 35/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 2.7222 - acc: 0.3495 - val_loss: 2.9715 - val_acc: 0.3895\n",
      "Epoch 36/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 2.6727 - acc: 0.3617 - val_loss: 2.9644 - val_acc: 0.3802\n",
      "Epoch 37/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 2.6236 - acc: 0.3689 - val_loss: 2.9063 - val_acc: 0.3858\n",
      "Epoch 38/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 2.5726 - acc: 0.3759 - val_loss: 2.8649 - val_acc: 0.3950\n",
      "Epoch 39/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 2.5901 - acc: 0.3739 - val_loss: 2.8465 - val_acc: 0.4061\n",
      "Epoch 40/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 2.4944 - acc: 0.4027 - val_loss: 2.8311 - val_acc: 0.3978\n",
      "Epoch 41/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 2.5058 - acc: 0.4043 - val_loss: 2.8468 - val_acc: 0.3996\n",
      "Epoch 42/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 2.4556 - acc: 0.4054 - val_loss: 2.8029 - val_acc: 0.4015\n",
      "Epoch 43/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 2.3972 - acc: 0.4216 - val_loss: 2.8406 - val_acc: 0.3904\n",
      "Epoch 44/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 2.3892 - acc: 0.4090 - val_loss: 2.7240 - val_acc: 0.4135\n",
      "Epoch 45/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 2.3480 - acc: 0.4153 - val_loss: 2.7725 - val_acc: 0.4098\n",
      "Epoch 46/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 2.3047 - acc: 0.4293 - val_loss: 2.7699 - val_acc: 0.4098\n",
      "Epoch 47/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 2.3006 - acc: 0.4226 - val_loss: 2.6986 - val_acc: 0.4274\n",
      "Epoch 48/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 2.2289 - acc: 0.4405 - val_loss: 2.7824 - val_acc: 0.4006\n",
      "Epoch 49/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 2.2503 - acc: 0.4378 - val_loss: 2.8374 - val_acc: 0.4043\n",
      "Epoch 50/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 2.2188 - acc: 0.4394 - val_loss: 2.6930 - val_acc: 0.4311\n",
      "Epoch 51/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 2.1393 - acc: 0.4625 - val_loss: 2.6748 - val_acc: 0.4274\n",
      "Epoch 52/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 2.1596 - acc: 0.4550 - val_loss: 2.6366 - val_acc: 0.4403\n",
      "Epoch 53/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 2.1037 - acc: 0.4712 - val_loss: 2.7303 - val_acc: 0.4292\n",
      "Epoch 54/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 2.0708 - acc: 0.4754 - val_loss: 2.6105 - val_acc: 0.4394\n",
      "Epoch 55/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 2.0613 - acc: 0.4789 - val_loss: 2.6104 - val_acc: 0.4459\n",
      "Epoch 56/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 2.0644 - acc: 0.4771 - val_loss: 2.5822 - val_acc: 0.4551\n",
      "Epoch 57/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 2.0156 - acc: 0.4897 - val_loss: 2.5934 - val_acc: 0.4468\n",
      "Epoch 58/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.9930 - acc: 0.4837 - val_loss: 2.5606 - val_acc: 0.4625\n",
      "Epoch 59/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.9405 - acc: 0.5005 - val_loss: 2.6981 - val_acc: 0.4366\n",
      "Epoch 60/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.9283 - acc: 0.5074 - val_loss: 2.6303 - val_acc: 0.4468\n",
      "Epoch 61/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.9213 - acc: 0.4992 - val_loss: 2.5661 - val_acc: 0.4681\n",
      "Epoch 62/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.9301 - acc: 0.5017 - val_loss: 2.5494 - val_acc: 0.4561\n",
      "Epoch 63/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.8846 - acc: 0.5209 - val_loss: 2.5938 - val_acc: 0.4607\n",
      "Epoch 64/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.8578 - acc: 0.5177 - val_loss: 2.4936 - val_acc: 0.4736\n",
      "Epoch 65/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.8137 - acc: 0.5283 - val_loss: 2.5999 - val_acc: 0.4635\n",
      "Epoch 66/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.8228 - acc: 0.5229 - val_loss: 2.6522 - val_acc: 0.4616\n",
      "Epoch 67/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.8016 - acc: 0.5218 - val_loss: 2.6140 - val_acc: 0.4477\n",
      "Epoch 68/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 1.8077 - acc: 0.5323 - val_loss: 2.4867 - val_acc: 0.4792\n",
      "Epoch 69/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 1.7612 - acc: 0.5454 - val_loss: 2.4901 - val_acc: 0.4718\n",
      "Epoch 70/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.7729 - acc: 0.5285 - val_loss: 2.5766 - val_acc: 0.4681\n",
      "Epoch 71/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.7176 - acc: 0.5551 - val_loss: 2.4394 - val_acc: 0.4829\n",
      "Epoch 72/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.7084 - acc: 0.5535 - val_loss: 2.4724 - val_acc: 0.4755\n",
      "Epoch 73/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.6458 - acc: 0.5702 - val_loss: 2.5267 - val_acc: 0.4653\n",
      "Epoch 74/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.6264 - acc: 0.5760 - val_loss: 2.5104 - val_acc: 0.4727\n",
      "Epoch 75/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.6957 - acc: 0.5485 - val_loss: 2.4669 - val_acc: 0.4699\n",
      "Epoch 76/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 1.6241 - acc: 0.5730 - val_loss: 2.4241 - val_acc: 0.4866\n",
      "Epoch 77/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.6241 - acc: 0.5686 - val_loss: 2.5370 - val_acc: 0.4820\n",
      "Epoch 78/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.6224 - acc: 0.5702 - val_loss: 2.5804 - val_acc: 0.4718\n",
      "Epoch 79/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 1.6170 - acc: 0.5664 - val_loss: 2.4642 - val_acc: 0.4857\n",
      "Epoch 80/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.6007 - acc: 0.5822 - val_loss: 2.4156 - val_acc: 0.4921\n",
      "Epoch 81/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.5851 - acc: 0.5793 - val_loss: 2.5821 - val_acc: 0.4727\n",
      "Epoch 82/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.5628 - acc: 0.5912 - val_loss: 2.4320 - val_acc: 0.4792\n",
      "Epoch 83/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.5210 - acc: 0.6048 - val_loss: 2.3884 - val_acc: 0.4884\n",
      "Epoch 84/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.5269 - acc: 0.5933 - val_loss: 2.4646 - val_acc: 0.4838\n",
      "Epoch 85/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.5100 - acc: 0.5955 - val_loss: 2.5566 - val_acc: 0.4801\n",
      "Epoch 86/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.4986 - acc: 0.5857 - val_loss: 2.5526 - val_acc: 0.4810\n",
      "Epoch 87/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 1.5066 - acc: 0.6055 - val_loss: 2.9393 - val_acc: 0.4644\n",
      "Epoch 88/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.4627 - acc: 0.6062 - val_loss: 2.3733 - val_acc: 0.4866\n",
      "Epoch 89/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 1.4974 - acc: 0.5929 - val_loss: 2.3809 - val_acc: 0.5042\n",
      "Epoch 90/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 1.4795 - acc: 0.6111 - val_loss: 2.3878 - val_acc: 0.5023\n",
      "Epoch 91/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.4618 - acc: 0.6043 - val_loss: 2.4451 - val_acc: 0.4995\n",
      "Epoch 92/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.4291 - acc: 0.6208 - val_loss: 2.5064 - val_acc: 0.4977\n",
      "Epoch 93/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 1.4293 - acc: 0.6137 - val_loss: 2.6013 - val_acc: 0.4736\n",
      "Epoch 94/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.4313 - acc: 0.6066 - val_loss: 2.4607 - val_acc: 0.4912\n",
      "Epoch 95/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 1.4141 - acc: 0.6211 - val_loss: 2.6514 - val_acc: 0.4986\n",
      "Epoch 96/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.3633 - acc: 0.6353 - val_loss: 2.4261 - val_acc: 0.5023\n",
      "Epoch 97/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 1.3821 - acc: 0.6242 - val_loss: 2.4935 - val_acc: 0.4958\n",
      "Epoch 98/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 1.3758 - acc: 0.6301 - val_loss: 2.3208 - val_acc: 0.5069\n",
      "Epoch 99/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.3837 - acc: 0.6235 - val_loss: 2.3072 - val_acc: 0.5208\n",
      "Epoch 100/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.3502 - acc: 0.6395 - val_loss: 2.5269 - val_acc: 0.5032\n",
      "Epoch 101/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 1.3532 - acc: 0.6303 - val_loss: 2.5226 - val_acc: 0.4986\n",
      "Epoch 102/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.2931 - acc: 0.6467 - val_loss: 2.6251 - val_acc: 0.4866\n",
      "Epoch 103/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.3149 - acc: 0.6467 - val_loss: 2.5290 - val_acc: 0.4949\n",
      "Epoch 104/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 1.2856 - acc: 0.6416 - val_loss: 2.4836 - val_acc: 0.5014\n",
      "Epoch 105/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 1.3171 - acc: 0.6422 - val_loss: 2.5516 - val_acc: 0.4958\n",
      "Epoch 106/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 1.2770 - acc: 0.6519 - val_loss: 2.5148 - val_acc: 0.4986\n",
      "Epoch 107/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 1.2348 - acc: 0.6651 - val_loss: 2.3481 - val_acc: 0.5023\n",
      "Epoch 108/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.3005 - acc: 0.6426 - val_loss: 2.5642 - val_acc: 0.5097\n",
      "Epoch 109/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 1.2195 - acc: 0.6600 - val_loss: 2.5441 - val_acc: 0.4921\n",
      "Epoch 110/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.2505 - acc: 0.6515 - val_loss: 2.4999 - val_acc: 0.5069\n",
      "Epoch 111/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.2710 - acc: 0.6537 - val_loss: 2.4500 - val_acc: 0.5125\n",
      "Epoch 112/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.2601 - acc: 0.6543 - val_loss: 2.5947 - val_acc: 0.5079\n",
      "Epoch 113/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.2298 - acc: 0.6612 - val_loss: 2.4565 - val_acc: 0.5180\n",
      "Epoch 114/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.2703 - acc: 0.6596 - val_loss: 2.5015 - val_acc: 0.5051\n",
      "Epoch 115/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 1.2311 - acc: 0.6556 - val_loss: 2.5013 - val_acc: 0.5060\n",
      "Epoch 116/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.2126 - acc: 0.6636 - val_loss: 2.5721 - val_acc: 0.5032\n",
      "Epoch 117/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.2365 - acc: 0.6657 - val_loss: 2.4624 - val_acc: 0.5143\n",
      "Epoch 118/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.1808 - acc: 0.6785 - val_loss: 2.6646 - val_acc: 0.5060\n",
      "Epoch 119/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.2059 - acc: 0.6665 - val_loss: 2.4728 - val_acc: 0.5060\n",
      "Epoch 120/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 1.1729 - acc: 0.6690 - val_loss: 2.7130 - val_acc: 0.4949\n",
      "Epoch 121/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 1.1989 - acc: 0.6743 - val_loss: 2.5328 - val_acc: 0.5125\n",
      "Epoch 122/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.2002 - acc: 0.6721 - val_loss: 2.4734 - val_acc: 0.5097\n",
      "Epoch 123/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.1917 - acc: 0.6720 - val_loss: 2.3699 - val_acc: 0.5273\n",
      "Epoch 124/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.1470 - acc: 0.6883 - val_loss: 2.5051 - val_acc: 0.5097\n",
      "Epoch 125/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.1826 - acc: 0.6761 - val_loss: 2.2838 - val_acc: 0.5273\n",
      "Epoch 126/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.1705 - acc: 0.6721 - val_loss: 2.3891 - val_acc: 0.5208\n",
      "Epoch 127/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.1479 - acc: 0.6775 - val_loss: 2.2592 - val_acc: 0.5143\n",
      "Epoch 128/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 1.1293 - acc: 0.6884 - val_loss: 2.4831 - val_acc: 0.5199\n",
      "Epoch 129/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.1001 - acc: 0.6893 - val_loss: 2.8676 - val_acc: 0.5106\n",
      "Epoch 130/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.0906 - acc: 0.6975 - val_loss: 2.2880 - val_acc: 0.5301\n",
      "Epoch 131/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.1266 - acc: 0.6852 - val_loss: 2.7104 - val_acc: 0.5097\n",
      "Epoch 132/200\n",
      "65/65 [==============================] - 273s 4s/step - loss: 1.1356 - acc: 0.6868 - val_loss: 2.3952 - val_acc: 0.5264\n",
      "Epoch 133/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.0999 - acc: 0.7001 - val_loss: 2.4734 - val_acc: 0.5254\n",
      "Epoch 134/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 1.1746 - acc: 0.6752 - val_loss: 2.7011 - val_acc: 0.5042\n",
      "Epoch 135/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.1323 - acc: 0.6877 - val_loss: 2.3285 - val_acc: 0.5291\n",
      "Epoch 136/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.0796 - acc: 0.6941 - val_loss: 2.3716 - val_acc: 0.5162\n",
      "Epoch 137/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.0727 - acc: 0.6978 - val_loss: 2.7781 - val_acc: 0.4977\n",
      "Epoch 138/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.1031 - acc: 0.6898 - val_loss: 2.4293 - val_acc: 0.5282\n",
      "Epoch 139/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.0757 - acc: 0.7060 - val_loss: 2.6241 - val_acc: 0.5190\n",
      "Epoch 140/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.0951 - acc: 0.6988 - val_loss: 2.4260 - val_acc: 0.5227\n",
      "Epoch 141/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 1.0648 - acc: 0.7092 - val_loss: 2.5781 - val_acc: 0.5217\n",
      "Epoch 142/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 1.0825 - acc: 0.6987 - val_loss: 2.3999 - val_acc: 0.5106\n",
      "Epoch 143/200\n",
      "65/65 [==============================] - 276s 4s/step - loss: 1.0759 - acc: 0.6923 - val_loss: 2.5435 - val_acc: 0.5162\n",
      "Epoch 144/200\n",
      "65/65 [==============================] - 282s 4s/step - loss: 1.0701 - acc: 0.7027 - val_loss: 2.4091 - val_acc: 0.5208\n",
      "Epoch 145/200\n",
      "65/65 [==============================] - 285s 4s/step - loss: 1.0716 - acc: 0.6990 - val_loss: 2.3229 - val_acc: 0.5291\n",
      "Epoch 146/200\n",
      "65/65 [==============================] - 279s 4s/step - loss: 1.0233 - acc: 0.7061 - val_loss: 2.6194 - val_acc: 0.5153\n",
      "Epoch 147/200\n",
      "65/65 [==============================] - 286s 4s/step - loss: 1.0460 - acc: 0.7093 - val_loss: 2.3125 - val_acc: 0.5347\n",
      "Epoch 148/200\n",
      "65/65 [==============================] - 281s 4s/step - loss: 1.0715 - acc: 0.7039 - val_loss: 2.7993 - val_acc: 0.5153\n",
      "Epoch 149/200\n",
      "65/65 [==============================] - 287s 4s/step - loss: 1.0527 - acc: 0.7062 - val_loss: 2.5562 - val_acc: 0.5180\n",
      "Epoch 150/200\n",
      "65/65 [==============================] - 290s 4s/step - loss: 1.0968 - acc: 0.6942 - val_loss: 2.5497 - val_acc: 0.5245\n",
      "Epoch 151/200\n",
      "65/65 [==============================] - 281s 4s/step - loss: 1.0660 - acc: 0.7119 - val_loss: 2.6406 - val_acc: 0.5208\n",
      "Epoch 152/200\n",
      "65/65 [==============================] - 283s 4s/step - loss: 1.0778 - acc: 0.6991 - val_loss: 2.3411 - val_acc: 0.5328\n",
      "Epoch 153/200\n",
      "65/65 [==============================] - 278s 4s/step - loss: 1.0171 - acc: 0.7234 - val_loss: 2.5479 - val_acc: 0.5236\n",
      "Epoch 154/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 1.0154 - acc: 0.7124 - val_loss: 2.6181 - val_acc: 0.5190\n",
      "Epoch 155/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 1.0156 - acc: 0.7297 - val_loss: 2.4049 - val_acc: 0.5162\n",
      "Epoch 156/200\n",
      "65/65 [==============================] - 276s 4s/step - loss: 1.0159 - acc: 0.7196 - val_loss: 2.5384 - val_acc: 0.5338\n",
      "Epoch 157/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 0.9772 - acc: 0.7173 - val_loss: 2.6532 - val_acc: 0.5190\n",
      "Epoch 158/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 1.0425 - acc: 0.7117 - val_loss: 2.3911 - val_acc: 0.5162\n",
      "Epoch 159/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 1.0472 - acc: 0.7053 - val_loss: 2.3154 - val_acc: 0.5106\n",
      "Epoch 160/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 1.0132 - acc: 0.7198 - val_loss: 2.5453 - val_acc: 0.5227\n",
      "Epoch 161/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 1.0006 - acc: 0.7168 - val_loss: 2.3644 - val_acc: 0.5356\n",
      "Epoch 162/200\n",
      "65/65 [==============================] - 276s 4s/step - loss: 1.0368 - acc: 0.7159 - val_loss: 2.7020 - val_acc: 0.5264\n",
      "Epoch 163/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 1.0069 - acc: 0.7219 - val_loss: 2.3640 - val_acc: 0.5217\n",
      "Epoch 164/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 1.0349 - acc: 0.7157 - val_loss: 2.4691 - val_acc: 0.5236\n",
      "Epoch 165/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 1.0157 - acc: 0.7206 - val_loss: 2.9652 - val_acc: 0.5208\n",
      "Epoch 166/200\n",
      "65/65 [==============================] - 277s 4s/step - loss: 1.0317 - acc: 0.7114 - val_loss: 2.5659 - val_acc: 0.5282\n",
      "Epoch 167/200\n",
      "65/65 [==============================] - 276s 4s/step - loss: 1.0056 - acc: 0.7161 - val_loss: 2.5225 - val_acc: 0.5273\n",
      "Epoch 168/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 0.9763 - acc: 0.7315 - val_loss: 2.3177 - val_acc: 0.5310\n",
      "Epoch 169/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 0.9669 - acc: 0.7338 - val_loss: 2.8657 - val_acc: 0.5291\n",
      "Epoch 170/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 0.9330 - acc: 0.7335 - val_loss: 2.2920 - val_acc: 0.5051\n",
      "Epoch 171/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 0.9988 - acc: 0.7235 - val_loss: 2.6158 - val_acc: 0.5227\n",
      "Epoch 172/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 0.9706 - acc: 0.7281 - val_loss: 2.4056 - val_acc: 0.5273\n",
      "Epoch 173/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 0.9690 - acc: 0.7336 - val_loss: 2.7288 - val_acc: 0.5125\n",
      "Epoch 174/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 0.9748 - acc: 0.7317 - val_loss: 2.4015 - val_acc: 0.5421\n",
      "Epoch 175/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 0.9588 - acc: 0.7362 - val_loss: 2.8567 - val_acc: 0.5153\n",
      "Epoch 176/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 0.9423 - acc: 0.7399 - val_loss: 3.0031 - val_acc: 0.5254\n",
      "Epoch 177/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 0.9550 - acc: 0.7259 - val_loss: 3.1598 - val_acc: 0.5143\n",
      "Epoch 178/200\n",
      "65/65 [==============================] - 276s 4s/step - loss: 0.9490 - acc: 0.7300 - val_loss: 2.4095 - val_acc: 0.5319\n",
      "Epoch 179/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 0.9581 - acc: 0.7335 - val_loss: 2.6677 - val_acc: 0.5282\n",
      "Epoch 180/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 0.9665 - acc: 0.7411 - val_loss: 2.5109 - val_acc: 0.5365\n",
      "Epoch 181/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 0.9804 - acc: 0.7211 - val_loss: 2.6416 - val_acc: 0.5393\n",
      "Epoch 182/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 0.9194 - acc: 0.7379 - val_loss: 2.5026 - val_acc: 0.5143\n",
      "Epoch 183/200\n",
      "65/65 [==============================] - 276s 4s/step - loss: 0.9649 - acc: 0.7302 - val_loss: 2.8811 - val_acc: 0.5190\n",
      "Epoch 184/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 0.9483 - acc: 0.7418 - val_loss: 2.6951 - val_acc: 0.5273\n",
      "Epoch 185/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 0.9465 - acc: 0.7273 - val_loss: 2.8507 - val_acc: 0.5365\n",
      "Epoch 186/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 0.9205 - acc: 0.7384 - val_loss: 2.3535 - val_acc: 0.5338\n",
      "Epoch 187/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 0.9950 - acc: 0.7205 - val_loss: 2.6397 - val_acc: 0.5375\n",
      "Epoch 188/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 0.9542 - acc: 0.7312 - val_loss: 3.0362 - val_acc: 0.5180\n",
      "Epoch 189/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 0.9424 - acc: 0.7431 - val_loss: 2.8194 - val_acc: 0.5282\n",
      "Epoch 190/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 0.9218 - acc: 0.7482 - val_loss: 2.6658 - val_acc: 0.5319\n",
      "Epoch 191/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 0.9436 - acc: 0.7434 - val_loss: 2.8439 - val_acc: 0.5180\n",
      "Epoch 192/200\n",
      "65/65 [==============================] - 274s 4s/step - loss: 0.9463 - acc: 0.7355 - val_loss: 2.8834 - val_acc: 0.5153\n",
      "Epoch 193/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 0.9565 - acc: 0.7288 - val_loss: 2.2792 - val_acc: 0.5171\n",
      "Epoch 194/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 0.9078 - acc: 0.7472 - val_loss: 2.7590 - val_acc: 0.5310\n",
      "Epoch 195/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 0.9524 - acc: 0.7298 - val_loss: 2.3617 - val_acc: 0.5190\n",
      "Epoch 196/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 0.9058 - acc: 0.7423 - val_loss: 2.5551 - val_acc: 0.5301\n",
      "Epoch 197/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 0.8953 - acc: 0.7447 - val_loss: 2.8701 - val_acc: 0.5236\n",
      "Epoch 198/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 0.9047 - acc: 0.7478 - val_loss: 2.4379 - val_acc: 0.5301\n",
      "Epoch 199/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 0.9769 - acc: 0.7338 - val_loss: 2.7210 - val_acc: 0.5282\n",
      "Epoch 200/200\n",
      "65/65 [==============================] - 275s 4s/step - loss: 0.9106 - acc: 0.7511 - val_loss: 2.4561 - val_acc: 0.5143\n",
      "Saved trained model at /home/kevin/Bittiger_Projects/saved_models/keras_lfw_trained_model_200.h5 \n",
      "1081/1081 [==============================] - 27s 25ms/step\n",
      "Test loss: 2.45611714699222\n",
      "Test accuracy: 0.5143385754482929\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), padding='same',\n",
    "                 input_shape=x_train.shape[1:]))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(32, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Conv2D(64, (3, 3), padding='same'))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Conv2D(64, (3, 3)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(512))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001, decay=1e-6)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer=opt,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255\n",
    "\n",
    "if not data_augmentation:\n",
    "    print('Not using data augmentation.')\n",
    "    model.fit(x_train, y_train,\n",
    "              batch_size=batch_size,\n",
    "              epochs=epochs,\n",
    "              validation_data=(x_test, y_test),\n",
    "              shuffle=True,  callbacks=[tb_callback])\n",
    "else:\n",
    "    print('Using real-time data augmentation.')\n",
    "    # This will do preprocessing and realtime data augmentation:\n",
    "    datagen = ImageDataGenerator(\n",
    "        featurewise_center=False,  # set input mean to 0 over the dataset\n",
    "        samplewise_center=False,  # set each sample mean to 0\n",
    "        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n",
    "        samplewise_std_normalization=False,  # divide each input by its std\n",
    "        zca_whitening=False,  # apply ZCA whitening\n",
    "        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n",
    "        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n",
    "        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n",
    "        horizontal_flip=True,  # randomly flip images\n",
    "        vertical_flip=False)  # randomly flip images\n",
    "\n",
    "    # Compute quantities required for feature-wise normalization\n",
    "    # (std, mean, and principal components if ZCA whitening is applied).\n",
    "    datagen.fit(x_train)\n",
    "\n",
    "    # Fit the model on the batches generated by datagen.flow().\n",
    "    model.fit_generator(datagen.flow(x_train, y_train,\n",
    "                                     batch_size=batch_size),\n",
    "                        epochs=epochs,\n",
    "                        validation_data=(x_test, y_test),\n",
    "                        workers=4, callbacks=[tb_callback])\n",
    "\n",
    "# Save model and weights\n",
    "if not os.path.isdir(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "model_path = os.path.join(save_dir, model_name)\n",
    "model.save(model_path)\n",
    "print('Saved trained model at %s ' % model_path)\n",
    "\n",
    "# Score trained model.\n",
    "scores = model.evaluate(x_test, y_test, verbose=1)\n",
    "print('Test loss:', scores[0])\n",
    "print('Test accuracy:', scores[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py27_faceRec]",
   "language": "python",
   "name": "conda-env-py27_faceRec-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
